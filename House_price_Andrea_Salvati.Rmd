---
title: "Individual Assignment - Advanced R"
output: 
  html_document:
    toc: true
    toc_depth: 3
author: Andrea Salvati
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(plyr)
library(dplyr)     # To compute the `union` of the levels.
library(png)       # To include images in this document.
library(knitr)     # To include images inline in this doc.
library(moments)   # Skewness
library(e1071)     # Alternative for Skewness
library(glmnet)    # Lasso
library(caret)     # To enable Lasso training with CV.
library(FSelector) # To use information gain for Feature Selection
library(data.table)
```


# Introduction

This assignment focuses on applying the Feature Engineering processes and the Evaluation methods to solve a practical scenario: Predict the price of houses.

# Data Reading and preparation

The dataset is offered in two separated fields, one for the training and another one for the test set. 

```{r Load Data}
original_training_data = read.csv(file = file.path("house_price_train.csv"))
original_test_data = read.csv(file = file.path("house_price_test.csv"))
```

To avoid applying the Feature Engineering process two times (once for training and once for test), I will join both datasets (using the `rbind` function), apply the FE and then split the datasets again. However, if I try to do join the two dataframes as they are, we will get an error because they do not have the same columns: `test_data` does not have a column `price`. Therefore, I first create this column in the test set and then we join the data

```{r Joinning datasets}
original_test_data$price <- 0
dataset <- as.data.table(rbind(original_training_data, original_test_data))
class(dataset)
```

Let's now visualize the dataset to see where to begin
```{r Dataset Visualization}
summary(dataset)
```

# Data Cleaning

In this section I am going to preform some data cleaning.

The feature `Id` present a unique value, consequently it is not going to offer any advantage for prediction.

```{r Dataset Visualization}
dataset[, c('id', 'date'):=NULL]
colnames(dataset)
```

## Factorize features

If we go back to the summary of the dataset we can identify some numerical features that are actually categories: What we have to do is to convert them to the proper 'class' or 'type' using the `as.factor` command.

```{r Dataset Visualization}
dataset$condition <- factor(dataset$condition)
dataset$grade <- factor(dataset$grade)
dataset$zipcode <- factor(dataset$grade)
#dataset$date <- as.Date.character(dataset$date, '%m/%d/%Y')

# lets now turn characters into factors
dataset[ ,names(dataset)[sapply(dataset, is.character)]:=lapply(.SD,as.factor),
           .SDcols = names(dataset)[sapply(dataset, is.character)]]
```

## Hunting NAs

Let's check if there are NAs in the dataset

```{r NAs discovery}
#Counting columns with null values.
na.cols <- which(colSums(is.na(dataset)) > 0)
paste('There are', length(na.cols), 'columns with missing values')
```

## Outliers

We will now focus on numerical values. If `NAs` where the natural enemy of categorical values, the main problem with numerical values are outliers (values which largely differ from the rest). Outliers can mislead the training of our models resulting in less accurate models and ultimately worse results.

In this section we seek to identify outliers to then properly deal with them. If we summarize the dataset, we can see variables which "Max." is much larger than the rest of values. These features are susceptible of containing outliers.

Here I compare the columns with outliers to the target variable (`price`) to visually check if there are some extreme values and just consider those as outliers.

```{r Outlier detection}
#Classify all numeric columns
numeric <- names(dataset[,sapply(dataset, is.numeric)])

#Select columns containing outliers with coef = 5
num_variables_containing_outliers <- c();
for (i in numeric){
    if(length(boxplot.stats(dataset$i, coef = 5)$out) != 0){
      num_variables_containing_outliers <- append(num_variables_containing_outliers, i);
    }
}

#Plot each variable against Sales Price
for (i in num_variables_containing_outliers){
  plot(dataset$i, dataset$price, type="p", xlab=i)
}
```

Now, looking at the graphs we can identify the outlier to remove and the threshold to apply.
```{r Outlier removal}
# check the original numebr of rows in the training set
a <- nrow(dataset[which(dataset$price!=0),])

# I am revoing the outlier just in the training set. This is why I have to specify the price different than 0
dataset <- dataset[-which(dataset[,"bathrooms"] > 7 & dataset[,"price"] !=0),]
dataset <- dataset[-which(dataset[,"sqft_above"] > 8000 & dataset[,"price"] !=0),]
dataset <- dataset[-which(dataset[,"sqft_basement"] > 4000 & dataset[,"price"] !=0),]
dataset <- dataset[-which(dataset[,"sqft_lot15"] > 8e+05 & dataset[,"price"] !=0),]

# check n rows removed
b <- nrow(dataset[which(dataset$price!=0),])

removed_rows <- a - b
print(removed_rows)
```

## Skewness

We now need to detect skewness in the Target value. Let's see what is the effect of skewness on a variable, and plot it using ggplot. The way of getting rid of the skewness is to use the `log` (or the `log1p`) of the values of that feature, to flatten it. To reduce right skewness, take roots or logarithms or reciprocals (x to 1/x). 

```{r}
df <- rbind(data.frame(version="price",x=original_training_data$price),
            data.frame(version="log(price+1)",x=log1p(original_training_data$price + 1)))

ggplot(data=df) +
  facet_wrap(~version,ncol=2,scales="free_x") +
  geom_histogram(aes(x=x), bins = 50)
```

We therefore transform the target value applying log
```{r Log transform the target for official scoring}
# Log transform the target for official scoring
dataset$price <- log1p(dataset$price)
```

The same "skewness" observed in the target variable also affects other variables. To facilitate the application of the regression model we are going to also eliminate this skewness. For numeric feature with excessive skewness, perform log transformation

I will set up my threshold for the skewness in 0.75. I place that value in that variable to adjust its value in a single place, in case I have to perform multiple tests.

```{r}
skewness_threshold = 1
```

Now, let's compute the skewness of each feature that is not 'factor' nor 'character'. So, I'm only interested in continuous values. One possible way of doing it is the following: First, lets determine what is the 'class' or data type of each of my features.

To do so, instead of `loops`, we will use the `apply` family of functions. They will __apply__ a method to each **row** or **column** of your dataset. It will depend on what to do specify as the first argument of the method. 

What we want to determine is the class of each column or feature, and to do so, we use the `class` method from R. We will pass the actual column or feature from our dataset (dataframe):

```{r}
column_types <- sapply(names(dataset), function(x) {
    class(dataset[[x]])
  }
)
numeric_columns <- names(column_types[column_types != "factor"])
numeric_columns
```

And now, with that information, we need to calculate the skewness of each column whose name is our list of __factor__ (or categorical) features. We use the `sapply` method again, to compute the skewness of each column whose name is in the list of `numeric_columns`.
```{r}
# skew of each variable
skew <- sapply(numeric_columns, function(x) { 
    e1071::skewness(dataset[[x]], na.rm = T)
  }
)
```

What we do need to make now is to apply the log to those whose skewness value is below a given threshold that we've set in 0.75. We should test different hypothesis with our threshold too.
```{r}
# transform all variables above a threshold skewness.
skew <- skew[abs(skew) > skewness_threshold]
for(x in names(skew)) {
  dataset[[x]] <- log(dataset[[x]] + 1)
}
```

# Feature Creation

In this section I will create some new features to improve the predictive power of the dataset.

```{r}
# having a look at the columns
colnames(dataset)
```

```{r}
dataset$TotalSqFeet <- as.numeric(dataset$sqft_basement + dataset$sqft_above + dataset$sqft_lot + dataset$sqft_living)
dataset$Remod <- ifelse(dataset$yr_built==dataset$yr_renovated, 0, 1) #0=No Remodeling, 1=Remodeling
```

```{r}
# We analyze the counting per zipcode:
sort(summary(dataset$zipcode), dec=T)/nrow(dataset)
p<-ggplot(dataset, aes(x=zipcode))+geom_bar(stat='count')+
  theme(axis.text.x = element_text(angle=45))
p
```

```{r}
# lets re-order the factor levels of make in decreasing order
dataset[, zipcode:=factor(zipcode, levels=names(sort(summary(dataset$zipcode), dec=T)))]
levels(dataset$zipcode)
```

```{r}
# We will create a label that will agregate into "others" those zipcode with less than 3% of share
niche_zipcode<-names(which(summary(dataset$zipcode)/nrow(dataset)<0.03))
niche_zipcode

dataset[, zipcode_agg:=as.factor(ifelse(zipcode%in%niche_zipcode,'others',as.character(zipcode)))]

summary(dataset$zipcode)/nrow(dataset)
summary(dataset$zipcode_agg)/nrow(dataset)
sum(summary(dataset$zipcode_agg)/nrow(dataset)) 

dataset[, length(levels(zipcode_agg))]
dataset[, length(levels(zipcode))]
dataset[, length(levels(zipcode_agg))/length(levels(zipcode))-1] # important reduction in factor cathegories


dataset[, zipcode_agg:=factor(zipcode_agg, levels=names(sort(summary(dataset$zipcode_agg), dec=T)))]
p<-ggplot(dataset, aes(x=zipcode_agg))+geom_bar(stat='count')+
  theme(axis.text.x = element_text(angle=45))
p

# we drop off the former zipcode variable
dataset[, zipcode:=NULL]
```


```{r}
# Now lets check the number of categories per factor variable
factor_variables<-names(dataset)[sapply(dataset, is.factor)]
count_factor_variables<-sapply(dataset[,factor_variables, with=F], summary)
count_factor_variables
```

```{r}
# Now lets check the number of categories per factor variable
data_ready<-caret::dummyVars(formula= ~., data = dataset, fullRank=T,sep = "_")
data_ready<-data.table(predict(data_ready, newdata = dataset))

names(data_ready)<-gsub('-','_',names(data_ready))
```

# Modelling

## Train, Validation Spliting

To facilitate the data cleaning and feature engineering we merged train and test datasets. We now split them again to create our final model.

```{r Train test split}
training_data <- data_ready[which(data_ready$price!=0),]
test <- data_ready[which(data_ready$price==0),]
```

We are going to split the annotated dataset in training and validation for the later evaluation of our regression models
```{r Train Validation split}
# I found this function, that is worth to save for future ocasions.
source('functions/Split_Datasets_function.R')
splits <- splitdf(training_data)
training <- splits$trainset
validation <- splits$testset
```

## Feature Selection
We here start the Feature Selection.

### Filtering Methods
We will rank the features according to their predictive power according to the methodologies seen in class: the Chi Squared Independence test and the Information Gain.

#### Full Model

Let's try first a baseline including all the features to evaluate the impact of the feature engineering.

```{r message=FALSE, warning=FALSE}
source('functions/regression_metrics.R')
source('functions/baseline.R')
lm.model(training, validation, "Baseline")
```



#### Information Gain Selection

Let's also try to use information gain to select the most important variables. In this section I will also use a cross validation procedure to select the best threshold for my model.
```{r cross validation information gain, message=FALSE, warning=FALSE}
source('functions/lm_cross_validation.R')
source('functions/baseline.R')
source('functions/information_gain_CV.R')

# using cross validation to find the optimal trashold
#information_gain_CV(training, validation, test, int_min = 0.01, int_max = 0.05, steps = 0.01)

# applying information gain with the optimal trashold found
weights<- data.frame(information.gain(price ~ ., training))
weights$feature <- rownames(weights)
weights[order(weights$attr_importance, decreasing = TRUE),]
information_gain_features <- weights$feature[weights$attr_importance > 0.005]

weights<- data.frame(information.gain(price ~ ., data = training[,c(information_gain_features,"price"),with=FALSE]))

ig_training <- training[,c(information_gain_features,"price"),with=FALSE]
ig_test <- test[,c(information_gain_features,"price"),with=FALSE]
ig_validation <- validation[,c(information_gain_features,"price"),with=FALSE]

lm.model(ig_training, ig_validation, "Information Gain")
```


#### Ridge Regression

Let's try the Ridge Regression
```{r Ridge Regression, warning=FALSE}
lambdas <- 10^seq(-3, 0, by = .05)

set.seed(121)
train_control_config <- trainControl(method = "repeatedcv", 
                                     number = 5, 
                                     repeats = 1,
                                     returnResamp = "all")

ridge.mod <- train(price ~ ., data = training, 
               method = "glmnet", 
               metric = "MAPE",
               trControl=train_control_config,
               tuneGrid = expand.grid(alpha = 0, lambda = lambdas))
```

##### Evaluation

Plotting the RMSE for the different lambda values, we can see the impact of this parameter in the model performance.
Small values seem to work better for this dataset.

```{r Ridge RMSE}
plot(ridge.mod)
```

Plotting the coefficients for different lambda values. As expected the larger the lambda (lower Norm) value the smaller the coefficients of the features. However, as we can see at the top of the features, there is no feature selection; i.e., the model always consider the 225 parameters.

```{r Ridge Coefficients}
plot(ridge.mod$finalModel)
```

```{r Ridge Evaluation}
ridge.mod.pred <- predict(ridge.mod, validation)
ridge.mod.pred[is.na(ridge.mod.pred)] <- 0

my_data <- as.data.frame(cbind(predicted=(exp(ridge.mod.pred) -1), observed=(exp(validation$price) -1)))
ridge.mod.mape <- sqrt(mean((ridge.mod.pred - validation$price)^2))
ridge.mod.price_error <- mean(abs((exp(ridge.mod.pred) -1) - (exp(validation$price) -1)))

ggplot(my_data, aes(predicted, observed)) +
    geom_point() + geom_smooth(method = "glm") +
    labs(x="Predicted") +
    ggtitle(ggtitle(paste("Ridge", ' MAPE: ', format(round(ridge.mod.mape, 4), nsmall=4), ' --> Price ERROR:', format(round(ridge.mod.price_error, 0), nsmall=0), 
                        '€', sep=''))) +  
    scale_x_continuous(labels = scales::comma) + 
    scale_y_continuous(labels = scales::comma)

```


Rank the variables according to the importance attributed by the model.
```{r}
# Print, plot variable importance
plot(varImp(ridge.mod), top = 20) # 20 most important features
```

### Lasso Regresion

Now Let's try lasso in combination with information gain. Moreover, I will use cross validation to identify the best number of seed
```{r Cross Validation Lasso Regression, warning=FALSE}
# cross validation
lasso_mape <- c()
lasso_index <- c()

for (i in seq(20,30,1)){
  
lambdas <- 10^seq(-3, 0, by = .05)

set.seed(i)
train_control_config <- trainControl(method = "repeatedcv", 
                                     number = 5, 
                                     repeats = 1,
                                     returnResamp = "all")

lasso.mod_cv <- train(price ~ ., data = ig_training, 
               method = "glmnet", 
               metric = "MAPE",
               trControl=train_control_config,
               tuneGrid = expand.grid(alpha = 1, lambda = lambdas))

lasso.mod.pred <- predict(lasso.mod_cv, validation)
lasso.mod.pred[is.na(lasso.mod.pred)] <- 0

my_data <- as.data.frame(cbind(predicted=(exp(lasso.mod.pred) -1), observed=(exp(validation$price) -1)))
lasso.mod.mape <- mape(lasso.mod.pred, validation$price)

lasso_mape <- c(lasso_mape, lasso.mod.mape)
lasso_index <- c(lasso_index, i)
}

lasso_cross_validation <- data.table(score=lasso_mape, indexes=lasso_index)
# optimal seed
lasso_seed <- lasso_cross_validation[score == min(score),][1,indexes]

```

```{r Lasso Regression, warning=FALSE}
# applying the lasso regression with the optimal seed value found above.
lambdas <- 10^seq(-3, 0, by = .05)

set.seed(lasso_seed)
train_control_config <- trainControl(method = "repeatedcv", 
                                     number = 5, 
                                     repeats = 1,
                                     returnResamp = "all")

lasso.mod <- train(price ~ ., data = training, 
               method = "glmnet", 
               metric = "MAPE",
               trControl=train_control_config,
               tuneGrid = expand.grid(alpha = 1, lambda = lambdas))

```

#### Evaluation

Plotting the RMSE for the different lambda values, we can see the impact of this parameter in the model performance.
Small values seem to work better for this dataset.

```{r lasso RMSE}
plot(lasso.mod)
```

Plotting the coefficients for different lambda values. As expected the larger the lambda (lower Norm) value the smaller the coefficients of the features. However, as we can see at the top of the features, there is no feature selection; i.e., the model always consider the 225 parameters.

```{r lasso Coefficients}
plot(lasso.mod$finalModel)
```

```{r lasso Evaluation}
# evaluating the model
lasso.mod.pred <- predict(lasso.mod, validation)
lasso.mod.pred[is.na(lasso.mod.pred)] <- 0

my_data <- as.data.frame(cbind(predicted=(exp(lasso.mod.pred) -1), observed=(exp(validation$price) -1)))
lasso.mod.mape <- mape(lasso.mod.pred, validation$price)
lasso.mod.price_error <- mean(abs((exp(lasso.mod.pred) -1) - (exp(validation$price) -1)))

ggplot(my_data, aes(predicted, observed)) +
    geom_point() + geom_smooth(method = "glm") +
    labs(x="Predicted") +
    ggtitle(ggtitle(paste("lasso", ' MAPE: ', format(round(lasso.mod.mape, 4), nsmall=4), ' --> Price ERROR:', format(round(lasso.mod.price_error, 0), nsmall=0), 
                        ' €', sep=''))) +  
    scale_x_continuous(labels = scales::comma) + 
    scale_y_continuous(labels = scales::comma)

```


Rank the variables according to the importance attributed by the model.
```{r}
# Print, plot variable importance
plot(varImp(lasso.mod), top = 20) # 20 most important features
```

# Final Submission
We splitted the original training data into train and validation to evaluate the candidate models. In order to generate the final submission we have to take instead all the data at our disposal.

In addition, we also applied a log transformation to the target variable, to revert this transformation you have to use the exp function.

In order to do my prediction I have tried all the combination of the models explained above. The best model is the lasso regression combined with information gain.

```{r Final Submission}
# Train the model using all the data
final.model <- lasso.mod

# Predict the prices for the test data (i.e., we use the exp function to revert the log transformation that we applied to the target variable)
final.pred <- as.numeric(exp(predict(final.model, test))-1) 
final.pred[is.na(final.pred)]
hist(final.pred, main="Histogram of Predictions", xlab = "Predictions")

lasso_submission <- data.frame(Id = original_test_data$id, price= (final.pred))
colnames(lasso_submission) <-c("Id", "SalePrice")
write.csv(lasso_submission, file = "submission1.csv", row.names = FALSE) 

```
