---
title: "Individual Assignment - Advanced R"
output: 
  html_document:
    toc: true
    toc_depth: 3
author: Andrea Salvati
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source('functions/load_libraries.R')
source('functions/Split_Datasets_function.R')
source('functions/regression_metrics.R')
source('functions/baseline.R')
source('functions/lm_cross_validation.R')
source('functions/information_gain_CV.R')
source('functions/lasso_CV.R')
formula<-as.formula(price~.) 
```


# Introduction

This assignment focuses on applying the Feature Engineering processes and the Evaluation methods to solve a practical scenario: Predict the price of houses.

# Data Reading and preparation

The dataset is offered in two separated fields, one for the training and another one for the test set. 

```{r Load Data}
original_training_data = read.csv(file = file.path("dataset/house_price_train.csv"))
original_test_data = read.csv(file = file.path("dataset/house_price_test.csv"))
```

To avoid applying the Feature Engineering process two times (once for training and once for test), I will join both datasets (using the `rbind` function), apply the FE and then split the datasets again. However, if I try to do join the two dataframes as they are, we will get an error because they do not have the same columns: `test_data` does not have a column `price`. Therefore, I first create this column in the test set and then we join the data

```{r Joinning datasets}
original_test_data$price <- 0
dataset <- as.data.table(rbind(original_training_data, original_test_data))
class(dataset)
```

Let's now visualize the dataset to see where to begin
```{r Dataset Visualization}
summary(dataset)
```

# Data Cleaning

In this section I am going to preform some data cleaning.

The feature `Id` and `deta` are not going to offer any advantage for prediction.

```{r Data cleaning}
dataset[, c('id', 'date'):=NULL]
colnames(dataset)
```

## Factorize features

If we go back to the summary of the dataset we can identify some numerical features that are actually categories: What we have to do is to convert them to the proper 'class' or 'type' using the `as.factor` command.

```{r Factorize features}
dataset$condition <- factor(dataset$condition)
dataset$grade <- factor(dataset$grade)
dataset$zipcode <- factor(dataset$zipcode)
dataset$yr_built <- factor(dataset$yr_built)
dataset$yr_renovated <- factor(dataset$yr_renovated)
dataset$view <- factor(dataset$view)

#dataset$date <- as.Date.character(dataset$date, '%m/%d/%Y')

# lets now turn characters into factors
dataset[ ,names(dataset)[sapply(dataset, is.character)]:=lapply(.SD,as.factor),
           .SDcols = names(dataset)[sapply(dataset, is.character)]]
```

## Hunting NAs

Let's check if there are NAs in the dataset

```{r NAs discovery}
#Counting columns with null values.
na.cols <- which(colSums(is.na(dataset)) > 0)
paste('There are', length(na.cols), 'columns with missing values')
```

## Outliers

We will now focus on numerical values. In this section I will detect numerical features which present outliers and I will clip those values.


```{r detecting Outliers}
# Classify all numeric columns
column_types <- sapply(names(dataset), function(x) {
    class(dataset[[x]])
  }
)
numeric_columns <- names(column_types[column_types != "factor"])

# Identify the the feature with outliers
outliers <- scores(dataset[,numeric_columns, with=FALSE], type="chisq", prob=0.9)
per_outliers <- sort(((colSums(outliers)/length(dataset$price))*100), decreasing = TRUE)
col_outliers_to_remove <- names(per_outliers[per_outliers > 5 & names(per_outliers) != 'lat' 
                                             & names(per_outliers) != 'long'])

# Plot the boxplots of the selected variables
for (i in col_outliers_to_remove){
  boxplot(dataset[,i, with=FALSE], type="p", xlab=i)
}
```

```{r Clipping Outliers}
# clipping the outliers
for (i in col_outliers_to_remove){
  qnt <- quantile(dataset[[i]], probs=c(.25, .75))
  caps <- quantile(dataset[[i]], probs=c(.05, .95))
  H <- 1.5 * IQR(dataset[[i]])
  dataset[[i]][dataset[[i]] < (qnt[1] - H)] <- caps[1]
  dataset[[i]][dataset[[i]] > (qnt[2] + H)] <- caps[2]
}

# Plot again the box plots to verify if the procedure has been succesful
for (i in col_outliers_to_remove){
  boxplot(dataset[,i, with=FALSE], type="p", xlab=i)
}
```

## Skewness

We now need to detect skewness in the Target value. Let's see what is the effect of skewness on a variable, and plot it using ggplot. The way of getting rid of the skewness is to use the `log` of the values of that feature, to flatten it.

```{r Skewness}
# plot the skewness of the target variable
df <- rbind(data.frame(version="price",x=original_training_data$price),
            data.frame(version="log(price+1)",x=log(original_training_data$price + 1)))

ggplot(data=df) +
  facet_wrap(~version,ncol=2,scales="free_x") +
  geom_histogram(aes(x=x), bins = 50)
```

We therefore transform the target value applying log

The same "skewness" observed in the target variable also affects other variables. To facilitate the application of the regression model we are going to also eliminate this skewness. For numeric feature with excessive skewness, perform log transformation

```{r Skewness threshold}
# setting up a threshold to decide to which numeric feature apply the log transformation
skewness_threshold = 1
```

Now, let's compute the skewness of each feature that is not 'factor' nor 'character'. 

```{r numeric_columns}
column_types <- sapply(names(dataset), function(x) {
    class(dataset[[x]])
  }
)
numeric_columns <- names(column_types[column_types != "factor"])
numeric_columns <- numeric_columns[numeric_columns != 'lat' & numeric_columns != 'long']
```

And now, with that information, we need to calculate the skewness of each column whose name is our list of __factor__ (or categorical) features.
```{r skew handling}
# skew of each variable
skew <- sapply(numeric_columns, function(x) { 
    e1071::skewness(dataset[[x]], na.rm = T)
  }
)
```

What we do need to make now is to apply the log to those whose skewness value is below a given threshold that we've set in 1.
```{r skew transformation}
# transform all variables above a threshold skewness.
skew <- skew[abs(skew) > skewness_threshold]
for(x in names(skew)) {
  dataset[[x]] <- log(dataset[[x]] + 1)
}
```

# Feature Creation

In this section I will create some new features to improve the predictive power of the dataset.

```{r}
# having a look at the columns
colnames(dataset)
```

```{r Feature Creation}
# creating a total square feet feature
dataset$TotalSqFeet <- as.numeric(dataset$sqft_basement + dataset$sqft_above + dataset$sqft_lot + dataset$sqft_living)
# Creating a Remodrnatoin variable (0 = No Remodeling, 1 = Remodeling)
dataset$Remod <- ifelse(as.numeric(dataset$yr_built)==as.numeric(dataset$yr_renovated), 0, 1)
dataset$Remod <- factor(dataset$Remod) 
```


```{r Others label - grade}
# We will create a label that will agregate into "others" those grade with less than 3% of share
niche_grade<-names(which(summary(dataset$grade)/nrow(dataset)<0.01))
niche_grade

dataset[, grade_agg:=as.factor(ifelse(grade%in%niche_grade,'others',as.character(grade)))]

summary(dataset$grade)/nrow(dataset)
summary(dataset$grade_agg)/nrow(dataset)
sum(summary(dataset$grade_agg)/nrow(dataset)) 

dataset[, length(levels(grade_agg))]
dataset[, length(levels(grade))]
dataset[, length(levels(grade_agg))/length(levels(grade))-1] # important reduction in factor cathegories


dataset[, grade_agg:=factor(grade_agg, levels=names(sort(summary(dataset$grade_agg), dec=T)))]
p<-ggplot(dataset, aes(x=grade_agg))+geom_bar(stat='count')+
  theme(axis.text.x = element_text(angle=45))
p

# we drop off the former grade variable
dataset <- dataset[, grade:=NULL]
```

```{r Categorical summary}
# Now lets check the number of categories per factor variable
factor_variables<-names(dataset)[sapply(dataset, is.factor)]
count_factor_variables<-sapply(dataset[,factor_variables, with=F], summary)
count_factor_variables
```

# Modelling

## Train, Validation Spliting

To facilitate the data cleaning and feature engineering we merged train and test datasets. We now split them again to create our final model.

```{r Train test split}
training_data <- dataset[which(dataset$price!=0),]
test <- dataset[which(dataset$price==0),]
```

We are going to split the annotated dataset in training and validation for the later evaluation of our regression models
```{r Train Validation split}
splits <- splitdf(training_data)
training <- splits$trainset
validation <- splits$testset
```

## Feature Selection

We here start the Feature Selection.

#### Full Model

Let's try first a baseline including all the features to evaluate the impact of the feature engineering.

```{r message=FALSE, warning=FALSE}
lm.model(training, validation, "Baseline")
```



#### Information Gain Selection

Let's try to use information gain to select the most important variables. In this section I will also use a cross validation procedure to select the best threshold for my model. As it is possible to notice the results are better than the baseline so from now on I will use the ig_training for all the models.

```{r cross validation information gain, message=FALSE, warning=FALSE}
# applying information gain with the optimal trashold found
weights<- data.frame(information.gain(formula, training))
weights$feature <- rownames(weights)
weights[order(weights$attr_importance, decreasing = TRUE),]
information_gain_features <- weights$feature[weights$attr_importance > information_gain_CV(training, validation,
                                                                                           int_min = 0.002, 
                                                                                           int_max = 0.006, 
                                                                                           steps = 0.01)]

ig_training <- training[,c(information_gain_features,"price"),with=FALSE]
ig_test <- test[,c(information_gain_features,"price"),with=FALSE]
ig_validation <- validation[,c(information_gain_features,"price"),with=FALSE]

lm.model(ig_training, ig_validation, "Information Gain")
```


### Ridge Regression

Let's try the Ridge Regression
```{r Ridge Regression, warning=FALSE}
lambdas <- 10^seq(-3, 0, by = .05)

set.seed(121)
train_control_config <- trainControl(method = "repeatedcv", 
                                     number = 5, 
                                     repeats = 1,
                                     returnResamp = "all")

ridge.mod <- train(formula, data = ig_training, 
               method = "glmnet", 
               metric = "RMSE",
               trControl=train_control_config,
               tuneGrid = expand.grid(alpha = 0, lambda = lambdas))
```

Plotting the RMSE for the different lambda values, we can see the impact of this parameter in the model performance.
Small values seem to work better for this dataset.

```{r Ridge RMSE}
plot(ridge.mod)
```

Plotting the coefficients for different lambda values. As expected the larger the lambda (lower Norm) value the smaller the coefficients of the features. However, as we can see at the top of the features, there is no feature selection; i.e., the model always consider the 225 parameters.

```{r Ridge Coefficients}
plot(ridge.mod$finalModel)
```

```{r Ridge Evaluation}
# evaluating the model 
ridge.mod.pred <- predict(ridge.mod, ig_validation)
ridge.mod.pred[is.na(ridge.mod.pred)] <- 0

my_data <- as.data.frame(cbind(predicted=(exp(ridge.mod.pred) -1), observed=(exp(validation$price) -1)))
ridge.mod.mape <- mape(ig_validation$price, ridge.mod.pred)
ridge.mod.price_error <- mean(abs((exp(ridge.mod.pred) -1) - (exp(validation$price) -1)))

# plottong the results
ggplot(my_data, aes(predicted, observed)) +
    geom_point() + geom_smooth(method = "glm") +
    labs(x="Predicted") +
    ggtitle(ggtitle(paste("Ridge", ' MAPE: ', format(round(ridge.mod.mape, 4), nsmall=4), ' --> Price ERROR:', format(round(ridge.mod.price_error, 0), nsmall=0), 
                        '€', sep=''))) +  
    scale_x_continuous(labels = scales::comma) + 
    scale_y_continuous(labels = scales::comma)

```


Rank the variables according to the importance attributed by the model.
```{r}
# Print, plot variable importance
plot(varImp(ridge.mod), top = 20) # 20 most important features
```

### Lasso Regresion

Now Let's try lasso in combination with information gain. Moreover, I will use cross validation to identify the best number of seed

```{r Lasso Regression, warning=FALSE}
# applying the lasso regression with the optimal seed value found above.
lambdas <- 10^seq(-3, 0, by = .05)

set.seed(lasso_CV(training, validation,10,60,10))
train_control_config <- trainControl(method = "repeatedcv", 
                                     number = 5, 
                                     repeats = 1,
                                     returnResamp = "all")

lasso.mod <- train(formula, data = ig_training, 
               method = "glmnet", 
               metric = "RMSE",
               trControl=train_control_config,
               tuneGrid = expand.grid(alpha = 1, lambda = lambdas))

```


Plotting the RMSE for the different lambda values, we can see the impact of this parameter in the model performance.
Small values seem to work better for this dataset.

```{r lasso RMSE}
plot(lasso.mod)
```

Plotting the coefficients for different lambda values. As expected the larger the lambda (lower Norm) value the smaller the coefficients of the features. However, as we can see at the top of the features, there is no feature selection; i.e., the model always consider the 225 parameters.

```{r lasso Coefficients}
plot(lasso.mod$finalModel)
```

```{r lasso Evaluation}
# evaluating the model
lasso.mod.pred <- predict(lasso.mod, ig_validation)
lasso.mod.pred[is.na(lasso.mod.pred)] <- 0

my_data <- as.data.frame(cbind(predicted=(exp(lasso.mod.pred) -1), observed=(exp(validation$price) -1)))
lasso.mod.mape <- mape(lasso.mod.pred, validation$price)
lasso.mod.price_error <- mean(abs((exp(lasso.mod.pred) -1) - (exp(validation$price) -1)))

# plotting the results
ggplot(my_data, aes(predicted, observed)) +
    geom_point() + geom_smooth(method = "glm") +
    labs(x="Predicted") +
    ggtitle(ggtitle(paste("lasso", ' MAPE: ', format(round(lasso.mod.mape, 4), nsmall=4), ' --> Price ERROR:', format(round(lasso.mod.price_error, 0), nsmall=0), 
                        ' €', sep=''))) +  
    scale_x_continuous(labels = scales::comma) + 
    scale_y_continuous(labels = scales::comma)

```


Rank the variables according to the importance attributed by the model.
```{r}
# Print, plot variable importance
plot(varImp(lasso.mod), top = 20) # 20 most important features
```

### Random Forest

Now Let's try Random Forest. After running all the models with the the defoult params, XGB was the best performing. For this reason I will apply the Hyper-parameters tuning just for xgb. 

```{r Random Forest, warning=FALSE}
# defining the model
rf_0<-ranger(formula=formula, data=ig_training)
rf.pred<-predict(rf_0, data = ig_validation)$predictions

# evaluation
rf.mape <- mape(rf.pred, validation$price)
rf.price_error <- mean(abs((exp(rf.pred) -1) - (exp(validation$price) -1)))
my_data <- as.data.frame(cbind(predicted=(exp(rf.pred) -1), observed=(exp(validation$price) -1)))

# plotting the results
ggplot(my_data, aes(predicted, observed)) +
    geom_point() + geom_smooth(method = "glm") +
    labs(x="Predicted") +
    ggtitle(ggtitle(paste("Random Forest", ' MAPE: ', format(round(rf.mape, 4), nsmall=4), ' --> Price ERROR:', format(round(rf.price_error, 0), nsmall=0), 
                        ' €', sep=''))) +  
    scale_x_continuous(labels = scales::comma) + 
    scale_y_continuous(labels = scales::comma)
```

### XGBoost

In this section we'll use xgb. First, we have to transform the data to allow xgb to work. We need to create dummies and to cast the data into a matrix

```{r XGBoost - Prep, warning=FALSE}
xgb_ig_training <- caret::dummyVars(formula= ~., data = ig_training, fullRank=T,sep = "_")
xgb_ig_training <- data.table(predict(xgb_ig_training, newdata = ig_training))
xgb_ig_training <- as.matrix(xgb_ig_training[, !'price', with=F])

xgb_ig_validation <- caret::dummyVars(formula= ~., data = ig_validation, fullRank=T,sep = "_")
xgb_ig_validation <- data.table(predict(xgb_ig_validation, newdata = ig_validation))
xgb_ig_validation <- as.matrix(xgb_ig_validation[, !'price', with=F])

xgb_ig_test <- caret::dummyVars(formula= ~., data = ig_test, fullRank=T,sep = "_")
xgb_ig_test <- data.table(predict(xgb_ig_test, newdata = ig_test))
xgb_ig_test <- as.matrix(xgb_ig_test[, !'price', with=F])

```

Now let's define our first model using all the default params

```{r XGBoost - model, warning=FALSE}
# defining the model
xgb_0<-xgboost(booster='gbtree',
               data=xgb_ig_training,
               label= training$price,
               nrounds = 100,
               objective='reg:linear')

xgb.pred<-predict(xgb_0, newdata = xgb_ig_validation, type='response')

```

```{r XGBoost - plot, warning=FALSE}
# evaluation
xgb.mape <- mape(xgb.pred, validation$price)
xgb.price_error <- mean(abs((exp(xgb.pred) -1) - (exp(validation$price) -1)))
my_data <- as.data.frame(cbind(predicted=(exp(xgb.pred) -1), observed=(exp(validation$price) -1)))

# Plotting the results
ggplot(my_data, aes(predicted, observed)) +
    geom_point() + geom_smooth(method = "glm") +
    labs(x="Predicted") +
    ggtitle(ggtitle(paste("XGBoost", ' MAPE: ', format(round(xgb.mape, 4), nsmall=4), ' --> Price ERROR:', format(round(xgb.price_error, 0), nsmall=0), 
                        ' €', sep=''))) +  
    scale_x_continuous(labels = scales::comma) + 
    scale_y_continuous(labels = scales::comma)
```

### XGB Optimized

To have a better performance I will use grid-search to find the optimal value of the function's hyperparameters.

```{r XGBoost Opt , warning=FALSE}
searchGridSubCol <- expand.grid(subsample = c(0.9, 1), 
                                colsample_bytree = c(0.5, 0.6),
                                max_depth = c(8, 9),
                                min_child = seq(1), 
                                eta = c(0.1, 0.2),
                                gamma = c(1)
)

ntrees <- 100

system.time(
rmseErrorsHyperparameters <- apply(searchGridSubCol, 1, function(parameterList){
  
  #Extract Parameters to test
  currentSubsampleRate <- parameterList[["subsample"]]
  currentColsampleRate <- parameterList[["colsample_bytree"]]
  currentDepth <- parameterList[["max_depth"]]
  currentEta <- parameterList[["eta"]]
  currentMinChild <- parameterList[["min_child"]]
  currentgamma <- parameterList[["gamma"]]
  xgboostModelCV <- xgb.cv(data =  xgb_ig_training, 
                           label = training$price, 
                           nrounds = ntrees, 
                           nfold = 5, 
                           showsd = TRUE, 
                           metrics = "rmse", 
                           verbose = TRUE, 
                           "eval_metric" = "rmse",
                           "objective" = "reg:linear", 
                           "max.depth" = currentDepth,
                           "eta" = currentEta, 
                           "gamma" = currentgamma,
                           "subsample" = currentSubsampleRate, 
                           "colsample_bytree" = currentColsampleRate, 
                           print_every_n = 10, 
                           "min_child_weight" = currentMinChild, 
                           booster = "gbtree",
                           early_stopping_rounds = 10)
  
  xvalidationScores <- as.data.frame(xgboostModelCV$evaluation_log)
  rmse <- tail(xvalidationScores$test_rmse_mean, 1)
  trmse <- tail(xvalidationScores$train_rmse_mean,1)
  output <- return(c(rmse, trmse, currentSubsampleRate, currentColsampleRate, currentDepth, currentEta, currentMinChild))}))
```

```{r XGBoost Opt - plot, warning=FALSE}
# finding the best combination of hyperparameters
output <- as.data.table(t(rmseErrorsHyperparameters))
varnames <- c("TestRMSE", "TrainRMSE", "SubSampRate", "ColSampRate", "Depth", "eta", "gamma")
names(output) <- varnames
optmimized_param <- output[TestRMSE == min(TestRMSE),3:7]
```

Now we will define the model with the best hyperparameters.

```{r XGBoost Opt - model, warning=FALSE}
# Defining the model 
xgb_opt<-xgboost(booster='gbtree',
               data=xgb_ig_training,
               label= training$price,
               nrounds = ntrees,
               max.depth = optmimized_param[,'Depth'],
               eta = optmimized_param[,'eta'],
               subsample = optmimized_param[,'SubSampRate'],
               colsample_bytree = optmimized_param[,'ColSampRate'],
               min_child_weight = 1,
               objective='reg:linear')

xgb.pred.opt<-predict(xgb_opt, newdata = xgb_ig_validation, type='response')
```


```{r XGBoost Opt - plot, warning=FALSE}
# evaluation
xgb.mape <- mape(xgb.pred.opt, validation$price)
xgb.price_error <- mean(abs((exp(xgb.pred.opt) -1) - (exp(validation$price) -1)))
my_data <- as.data.frame(cbind(predicted=(exp(xgb.pred.opt) -1), observed=(exp(validation$price) -1)))

# plotting the results
ggplot(my_data, aes(predicted, observed)) +
    geom_point() + geom_smooth(method = "glm") +
    labs(x="Predicted") +
    ggtitle(ggtitle(paste("XGBoost", ' MAPE: ', format(round(xgb.mape, 4), nsmall=4), ' --> Price ERROR:', format(round(xgb.price_error, 0), nsmall=0), 
                        ' €', sep=''))) +  
    scale_x_continuous(labels = scales::comma) + 
    scale_y_continuous(labels = scales::comma)
```

### Regression with StepWise feature selection 

In this secion we'll try to apply Regression with StepWise feature selection. 

```{r Regression with StepWise feature selection, warning=FALSE}
# adapting the dataset to work for the algorithm
lm_ig_training <- ig_training
lm_ig_training$yr_built <- as.numeric(lm_ig_training$yr_built)
lm_ig_training$yr_renovated <- as.numeric(lm_ig_training$yr_renovated)

lm_ig_validation <- ig_validation
lm_ig_validation$yr_built <- as.numeric(lm_ig_validation$yr_built)
lm_ig_validation$yr_renovated <- as.numeric(lm_ig_validation$yr_renovated)

#### Regression with StepWise feature selection 
lm_0<-stepAIC(lm(formula = formula, 
                 data=lm_ig_training),
              trace=F)

lm.pred<-predict(lm_0, newdata = lm_ig_validation)

# evaluation
lm.mape <- mape(lm.pred, validation$price)
lm.price_error <- mean(abs((exp(lm.pred) -1) - (exp(validation$price) -1)))
my_data <- as.data.frame(cbind(predicted=(exp(lm.pred) -1), observed=(exp(validation$price) -1)))

# plotting the results
ggplot(my_data, aes(predicted, observed)) +
    geom_point() + geom_smooth(method = "glm") +
    labs(x="Predicted") +
    ggtitle(ggtitle(paste("StepWise", ' MAPE: ', format(round(lm.mape, 4), nsmall=4), ' --> Price ERROR:', format(round(lm.price_error, 0), nsmall=0), 
                        ' €', sep=''))) +  
    scale_x_continuous(labels = scales::comma) + 
    scale_y_continuous(labels = scales::comma)
```

### Regression with regularization

in this section wel'll try to apply Regression with regularization

```{r Regression with regularization, warning=FALSE}
# Defining the model
glmnet_cv<-cv.glmnet(x = xgb_ig_training,
                     nfolds = 5,
                     y = training[['price']],
                     alpha=1,
                     family = 'gaussian',
                     standardize = T)
plot.cv.glmnet(glmnet_cv)
glmnet_cv$lambda.min

glmnet_0<-glmnet(x = xgb_ig_training, 
                 y = training[['price']],
                 family = 'gaussian',
                 alpha=1, lambda = glmnet_cv$lambda.min)
glmnet_0$beta
glmnet.pred <- predict(glmnet_0, newx = xgb_ig_validation)

# Evaluation
glmnet.mape <- mape(glmnet.pred, validation$price)
glmnetprice_error <- mean(abs((exp(glmnet.pred) -1) - (exp(validation$price) -1)))
my_data <- as.data.frame(cbind(predicted=(exp(glmnet.pred) -1), observed=(exp(validation$price) -1)))

# Plotting the results
ggplot(my_data, aes(s0, observed)) +
    geom_point() + geom_smooth(method = "glm") +
    labs(x="Predicted") +
    ggtitle(ggtitle(paste("Regression with regularization", ' MAPE: ', format(round(glmnet.mape, 4), nsmall=4), ' --> Price ERROR:', format(round(glmnetprice_error, 0), nsmall=0), 
                        ' €', sep=''))) +  
    scale_x_continuous(labels = scales::comma) + 
    scale_y_continuous(labels = scales::comma)
```

# Final Submission
We splitted the original training data into train and validation to evaluate the candidate models. In order to generate the final submission we have to take instead all the data at our disposal.

In addition, we also applied a log transformation to the target variable, to revert this transformation you have to use the exp function.

In order to do my prediction I have tried all the combination of the models explained above. The best model is the optimized xgb.

```{r Final Submission}
# Train the model using all the data
final.model <- xgb_opt

# Predict the prices for the test data (i.e., we use the exp function to revert the log transformation that we applied to the target variable)
final.pred <- as.numeric(exp(predict(final.model, xgb_ig_test, type='response'))-1)
final.pred[is.na(final.pred)]
hist(final.pred, main="Histogram of Predictions", xlab = "Predictions")

xgb_submission <- data.frame(Id = original_test_data$id, price= (final.pred))
colnames(xgb_submission) <-c("Id", "price")
write.csv(xgb_submission, file = "submission1.csv", row.names = FALSE) 

```